{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743e3665-1b47-4d2c-bb64-bc8f777f819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93390b86-6ef4-4ff1-bb3a-53d1cb22d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: datasets in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (3.1.0)\n",
      "Collecting unsloth\n",
      "  Using cached unsloth-2024.11.7-py3-none-any.whl.metadata (59 kB)\n",
      "Requirement already satisfied: filelock in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: psutil in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from datasets) (3.11.2)\n",
      "Collecting unsloth-zoo>=2024.11.1 (from unsloth)\n",
      "  Using cached unsloth_zoo-2024.11.5-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.28.post3.tar.gz (7.8 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2024.11.6-py3-none-any.whl.metadata (59 kB)\n",
      "  Downloading unsloth-2024.11.5-py3-none-any.whl.metadata (59 kB)\n",
      "  Downloading unsloth-2024.11.4-py3-none-any.whl.metadata (59 kB)\n",
      "  Downloading unsloth-2024.11.2-py3-none-any.whl.metadata (59 kB)\n",
      "  Downloading unsloth-2024.10.7-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.6-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.5-py3-none-any.whl.metadata (56 kB)\n",
      "INFO: pip is still looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading unsloth-2024.10.4-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.2-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.1-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.0-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.9.post4-py3-none-any.whl.metadata (56 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading unsloth-2024.9.post3-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9.post2-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9.post1-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9-py3-none-any.whl.metadata (54 kB)\n",
      "Collecting xformers==0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.27.post2.tar.gz (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unsloth\n",
      "  Downloading unsloth-2024.8-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/avishekchoudhury/miniconda3/envs/llama_env/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2024.8-py3-none-any.whl (136 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unsloth, scipy, safetensors, regex, bitsandbytes, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.1.1 bitsandbytes-0.42.0 peft-0.13.2 regex-2024.11.6 safetensors-0.4.5 scipy-1.14.1 tokenizers-0.20.3 transformers-4.46.3 unsloth-2024.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers peft accelerate bitsandbytes datasets unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433d2f46-bae4-4216-8f3d-2f40827d5605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a42e1b2eb41029f648e9ae2ccaf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Unsloth's SFTTrainer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Optimization libraries\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_env/lib/python3.10/site-packages/unsloth/__init__.py:87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Torch 2.5 has including_emulation\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m SUPPORTS_BFLOAT16 \u001b[38;5;241m=\u001b[39m (major_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (major_torch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (minor_torch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m): \n",
      "File \u001b[0;32m~/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/cuda/__init__.py:509\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/cuda/__init__.py:523\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Hugging Face libraries\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LoRA and PEFT (Parameter-Efficient Fine-Tuning)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Unsloth's SFTTrainer\n",
    "from unsloth import SFTTrainer\n",
    "\n",
    "# Optimization libraries\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b3f435e-18f5-411a-a03f-20290918a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6737060f7ef9698051fcd9fa', 'name': 'aviici4cs', 'fullname': 'Avishek Choudhury', 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/q-CEAmZndINHimWFRECay.png', 'orgs': [{'type': 'org', 'id': '64dac823a1182ee16c3a4788', 'name': 'universityofutah', 'fullname': 'University of Utah', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64dac67c7f749b6e34428a9b/xrHJ5kszSnoTAhNeSzaw-.png', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'IndependentLLM', 'role': 'fineGrained', 'createdAt': '2024-11-18T08:08:02.813Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['inference.serverless.write', 'discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '6737060f7ef9698051fcd9fa', 'type': 'user', 'name': 'aviici4cs'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write', 'user.billing.read']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "login(token=\"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\")\n",
    "user_info = whoami()\n",
    "print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9fe85-2c5d-41fc-a613-4ed300493a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create prompt for training the model\n",
    "\n",
    "def create_prompt(instruction, input_text, output_text):\n",
    "    prompt = (\n",
    "        f\"<|begin_of_text|>\"\n",
    "        f\"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{instruction}\\n\"\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"{input_text}\\n\"\n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{output_text}\\n\"\n",
    "        f\"<|eot_id|>\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74935f-8606-4807-acc3-99dfdf97fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_for_training(file_path, output_file):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_columns = {\"instruction\", \"input\", \"output\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"The dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Generate prompts\n",
    "    for _, row in df.iterrows():\n",
    "        instruction = row[\"instruction\"]\n",
    "        input_text = row[\"input\"]\n",
    "        output_text = row[\"output\"]\n",
    "        prompt = create_prompt_for_dataset(instruction, input_text, output_text)\n",
    "        json.dump({\"text\": prompt}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    # Save prompts to the output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for prompt in prompts:\n",
    "            f.write(prompt + \"\\n\\n\")  # Add spacing between prompts for clarity\n",
    "\n",
    "    print(f\"Processed {len(prompts)} prompts. Saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8774509-e1c1-4772-b2d3-37b401750398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "datasetPath = \"../processedData\"  # Replace with your dataset file path\n",
    "output_file = \"prompts.txt\"  # Replace with your desired output file path\n",
    "\n",
    "process_dataset(dataset_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
